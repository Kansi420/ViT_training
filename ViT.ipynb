{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pinecone-io/examples/blob/master/learn/image-retrieval/vision-transformers/vit.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: transformers in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (4.28.1)\n",
      "Requirement already satisfied: torch in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (2.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (2023.4.0)\n",
      "Requirement already satisfied: pandas in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: aiohttp in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: responses<0.19 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: packaging in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: xxhash in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: filelock in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: networkx in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: sympy in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: jinja2 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: typing-extensions in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: wheel in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.6.3)\n",
      "Requirement already satisfied: lit in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.3)\n",
      "Requirement already satisfied: cmake in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages/datasets/load.py:1748: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=all_checks' instead.\n",
      "  warnings.warn(\n",
      "Found cached dataset cifar10 (/home/nlp-lab/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    }
   ],
   "source": [
    "# import CIFAR-10 dataset from HuggingFace\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_train = load_dataset(\n",
    "    'cifar10',\n",
    "    split='train',\n",
    "    ignore_verifications=False # set to True if seeing splits Error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': Image(decode=True, id=None),\n",
       " 'label': ClassLabel(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], id=None)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages/datasets/load.py:1748: FutureWarning: 'ignore_verifications' was deprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'verification_mode=no_checks' instead.\n",
      "  warnings.warn(\n",
      "Found cached dataset cifar10 (/home/nlp-lab/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['img', 'label'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test = load_dataset(\n",
    "    'cifar10',\n",
    "    split='test', # testing split\n",
    "    ignore_verifications=True\n",
    ")\n",
    "\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(dataset_train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], id=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " ClassLabel(names=['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'], id=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(set(dataset_train['label']))\n",
    "labels = dataset_train.features['label']\n",
    "num_classes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'img': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>,\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]['img']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'airplane')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]['label'], labels.names[dataset_train[0]['label']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Loading ViT Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "#import model\n",
    "model_id = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
    "    model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTFeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[ 0.3961,  0.3961,  0.3961,  ...,  0.2941,  0.2941,  0.2941],\n",
       "          [ 0.3961,  0.3961,  0.3961,  ...,  0.2941,  0.2941,  0.2941],\n",
       "          [ 0.3961,  0.3961,  0.3961,  ...,  0.2941,  0.2941,  0.2941],\n",
       "          ...,\n",
       "          [-0.1922, -0.1922, -0.1922,  ..., -0.2863, -0.2863, -0.2863],\n",
       "          [-0.1922, -0.1922, -0.1922,  ..., -0.2863, -0.2863, -0.2863],\n",
       "          [-0.1922, -0.1922, -0.1922,  ..., -0.2863, -0.2863, -0.2863]],\n",
       "\n",
       "         [[ 0.3804,  0.3804,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
       "          [ 0.3804,  0.3804,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
       "          [ 0.3804,  0.3804,  0.3804,  ...,  0.2784,  0.2784,  0.2784],\n",
       "          ...,\n",
       "          [-0.2471, -0.2471, -0.2471,  ..., -0.3412, -0.3412, -0.3412],\n",
       "          [-0.2471, -0.2471, -0.2471,  ..., -0.3412, -0.3412, -0.3412],\n",
       "          [-0.2471, -0.2471, -0.2471,  ..., -0.3412, -0.3412, -0.3412]],\n",
       "\n",
       "         [[ 0.4824,  0.4824,  0.4824,  ...,  0.3647,  0.3647,  0.3647],\n",
       "          [ 0.4824,  0.4824,  0.4824,  ...,  0.3647,  0.3647,  0.3647],\n",
       "          [ 0.4824,  0.4824,  0.4824,  ...,  0.3647,  0.3647,  0.3647],\n",
       "          ...,\n",
       "          [-0.2784, -0.2784, -0.2784,  ..., -0.3961, -0.3961, -0.3961],\n",
       "          [-0.2784, -0.2784, -0.2784,  ..., -0.3961, -0.3961, -0.3961],\n",
       "          [-0.2784, -0.2784, -0.2784,  ..., -0.3961, -0.3961, -0.3961]]]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Passing the Airplane tensor through the feature extractor \n",
    "example = feature_extractor(\n",
    "    dataset_train[0]['img'],\n",
    "    return_tensors='pt'\n",
    ")\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 30 21:36:31 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:65:00.0 Off |                  N/A |\n",
      "| 34%   46C    P8    28W / 320W |     13MiB / 10240MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:B3:00.0  On |                  N/A |\n",
      "| 40%   54C    P8    33W / 320W |    358MiB / 10240MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2649      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A      3538      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A      2649      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    1   N/A  N/A      3538      G   /usr/lib/xorg/Xorg                140MiB |\n",
      "|    1   N/A  N/A      3891      G   /usr/bin/gnome-shell               15MiB |\n",
      "|    1   N/A  N/A    144994      G   ...050161767643829688,131072       36MiB |\n",
      "|    1   N/A  N/A    145632      G   ...RendererForSitePerProcess      111MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    # take a list of PIL images and turn them to pixel values\n",
    "    inputs = feature_extractor(\n",
    "        batch['img'],\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    #include the labels\n",
    "    inputs['label'] = batch['label']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply this to both the training and testing dataset\n",
    "\n",
    "# transform the training dataset\n",
    "prepared_train = dataset_train.with_transform(preprocess)\n",
    "# ... and the testing dataset\n",
    "prepared_test = dataset_test.with_transform(preprocess)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, whenever you get an example from the dataset, the transform will be applied in real time (on both samples and slices)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fine-Tuning\n",
    "\n",
    "In this section, we are going to build the Trainer, which is a feature-complete training and eval loop for PyTorch, optimized for HuggingFace ðŸ¤— Transformers.\n",
    "<br>\n",
    "We need to define all of the arguments that it will include:\n",
    "<br>\n",
    "- training and testing dataset\n",
    "- feature extractor\n",
    "- model\n",
    "- collate function\n",
    "- evaluation metric\n",
    "<br>\n",
    "\n",
    "... other training arguments.\n",
    "The collate function is useful when dealing with lots of data. Batches are lists of dictionaries, so collate will help us create batch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the evaluation metric we are going to use to compare prediction with actual labels. We will use the accuracy evaluation metric.\n",
    "<br>\n",
    "Accuracy is defined as the proportion of correct predictions (True Positive (\n",
    ") and True Negative (\n",
    ")) among the total number of cases processed (\n",
    ", \n",
    ", False Positive (\n",
    "), and False Negative (\n",
    ")).\n",
    "\n",
    " \n",
    "Below, we are using accuracy within the compute_metrics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1483204/3243213926.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "# accuracy metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(\n",
    "        predictions=np.argmax(p.predictions, axis=1),\n",
    "        references=p.label_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./cifar\",\n",
    "  per_device_train_batch_size=16,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=4,\n",
    "  save_steps=100,\n",
    "  eval_steps=100,\n",
    "  logging_steps=10,\n",
    "  learning_rate=2e-4,\n",
    "  save_total_limit=2,\n",
    "  remove_unused_columns=False,\n",
    "  push_to_hub=False,\n",
    "  load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the pre-trained model. We'll add num_labels on init so the model creates a classification head with the right number of units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "labels = dataset_train.features['label'].names\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_id,  # classification head\n",
    "    num_labels=len(labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the characteristics of our model.\n",
    "\n",
    "Now, all instances can be passed to Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train,\n",
    "    eval_dataset=prepared_test,\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (0.15.3)\n",
      "Requirement already satisfied: pathtools in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (2.28.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: typing-extensions in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (65.6.3)\n",
      "Requirement already satisfied: setproctitle in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (1.2.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (1.19.1)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: PyYAML in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saved Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp-lab/anaconda3/envs/open_ai/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mai21mtech14008\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nlp-lab/prad_project/ViT_training/wandb/run-20230530_213702-cjfpnjkx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ai21mtech14008/huggingface/runs/cjfpnjkx' target=\"_blank\">charmed-meadow-7</a></strong> to <a href='https://wandb.ai/ai21mtech14008/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ai21mtech14008/huggingface' target=\"_blank\">https://wandb.ai/ai21mtech14008/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ai21mtech14008/huggingface/runs/cjfpnjkx' target=\"_blank\">https://wandb.ai/ai21mtech14008/huggingface/runs/cjfpnjkx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "# save tokenizer with the model\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "# save the trainer state\n",
    "trainer.save_state()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation\n",
    "\n",
    "We can now evaluate our model using the accuracy metric defined above..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# show the first image of the testing dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m image \u001b[39m=\u001b[39m dataset_test[\u001b[39m\"\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mresize((\u001b[39m200\u001b[39m,\u001b[39m200\u001b[39m))\n\u001b[1;32m      3\u001b[0m image\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset_test' is not defined"
     ]
    }
   ],
   "source": [
    "# show the first image of the testing dataset\n",
    "image = dataset_test[\"img\"][0].resize((200,200))\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the actual label of the first image of the testing dataset\n",
    "actual_label = dataset_test[\"label\"][0]\n",
    "\n",
    "labels = dataset_test.features['label']\n",
    "actual_label, labels.names[actual_label]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the image represents a cat. Let's now see what our model has predicted. Given we saved it on the HuggingFace Hub, we first need to import it. We can use ViTForImageClassification and ViTFeatureExtractor to import the model and extract its features. We would need the predicted pixel values \"pt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "\n",
    "# import our fine-tuned model\n",
    "model_name_or_path = 'LaCarnevali/vit-cifar10'\n",
    "model_finetuned = ViTForImageClassification.from_pretrained(model_name_or_path)\n",
    "# import features\n",
    "feature_extractor_finetuned = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = feature_extractor_finetuned(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model_finetuned(**inputs).logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see what is our predicted label. Do extract it, we can use the argmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = logits.argmax(-1).item()\n",
    "labels = dataset_test.features['label']\n",
    "labels.names[predicted_label]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "Article\n",
    "\n",
    "[1] Dosovitskiy et al., An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale, 2021, CV.\n",
    "\n",
    "[2] Vaswani et al., Attention Is All You Need, 2017.\n",
    "\n",
    "[3] Saeed M., A Gentle Introduction to Positional Encoding in Transformer Models, Part 1, 2022, Attention, Machine Learning Mastery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vicuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
